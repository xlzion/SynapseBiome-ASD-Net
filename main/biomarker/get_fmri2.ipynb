{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2848027/3685911741.py:273: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"fmri_gnn.pth\", map_location=device)\n",
      "/home/yangzongxian/anaconda3/envs/xlz1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "计算ROI重要性:   0%|          | 0/23 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:   4%|▍         | 1/23 [00:01<00:42,  1.93s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:   9%|▊         | 2/23 [00:02<00:19,  1.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  13%|█▎        | 3/23 [00:03<00:18,  1.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  22%|██▏       | 5/23 [00:04<00:15,  1.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n",
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  26%|██▌       | 6/23 [00:06<00:18,  1.07s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  30%|███       | 7/23 [00:07<00:15,  1.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  35%|███▍      | 8/23 [00:07<00:10,  1.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  39%|███▉      | 9/23 [00:08<00:12,  1.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  48%|████▊     | 11/23 [00:11<00:13,  1.13s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n",
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  52%|█████▏    | 12/23 [00:12<00:11,  1.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  57%|█████▋    | 13/23 [00:12<00:08,  1.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  61%|██████    | 14/23 [00:13<00:08,  1.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  65%|██████▌   | 15/23 [00:14<00:08,  1.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  70%|██████▉   | 16/23 [00:16<00:07,  1.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  74%|███████▍  | 17/23 [00:17<00:06,  1.12s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  78%|███████▊  | 18/23 [00:17<00:04,  1.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  87%|████████▋ | 20/23 [00:18<00:01,  1.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n",
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性:  91%|█████████▏| 21/23 [00:19<00:01,  1.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([32, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算ROI重要性: 100%|██████████| 23/23 [00:20<00:00,  1.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度检查: torch.Size([19, 40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 100 fMRI ROIs (按索引显示):\n",
      "1. ROI_094 重要性分数: 0.2219\n",
      "2. ROI_036 重要性分数: 0.2193\n",
      "3. ROI_102 重要性分数: 0.2187\n",
      "4. ROI_071 重要性分数: 0.2182\n",
      "5. ROI_038 重要性分数: 0.2170\n",
      "6. ROI_057 重要性分数: 0.2141\n",
      "7. ROI_031 重要性分数: 0.2134\n",
      "8. ROI_068 重要性分数: 0.2126\n",
      "9. ROI_013 重要性分数: 0.2125\n",
      "10. ROI_025 重要性分数: 0.2120\n",
      "11. ROI_073 重要性分数: 0.2118\n",
      "12. ROI_080 重要性分数: 0.2115\n",
      "13. ROI_010 重要性分数: 0.2114\n",
      "14. ROI_017 重要性分数: 0.2107\n",
      "15. ROI_064 重要性分数: 0.2105\n",
      "16. ROI_059 重要性分数: 0.2097\n",
      "17. ROI_104 重要性分数: 0.2081\n",
      "18. ROI_043 重要性分数: 0.2072\n",
      "19. ROI_099 重要性分数: 0.2065\n",
      "20. ROI_048 重要性分数: 0.2063\n",
      "21. ROI_001 重要性分数: 0.2059\n",
      "22. ROI_079 重要性分数: 0.2050\n",
      "23. ROI_030 重要性分数: 0.2044\n",
      "24. ROI_083 重要性分数: 0.2043\n",
      "25. ROI_158 重要性分数: 0.2042\n",
      "26. ROI_028 重要性分数: 0.2041\n",
      "27. ROI_161 重要性分数: 0.2039\n",
      "28. ROI_026 重要性分数: 0.2037\n",
      "29. ROI_009 重要性分数: 0.2036\n",
      "30. ROI_103 重要性分数: 0.2032\n",
      "31. ROI_077 重要性分数: 0.2031\n",
      "32. ROI_067 重要性分数: 0.2030\n",
      "33. ROI_070 重要性分数: 0.2029\n",
      "34. ROI_046 重要性分数: 0.2027\n",
      "35. ROI_004 重要性分数: 0.2025\n",
      "36. ROI_177 重要性分数: 0.2020\n",
      "37. ROI_141 重要性分数: 0.2013\n",
      "38. ROI_183 重要性分数: 0.2012\n",
      "39. ROI_117 重要性分数: 0.2008\n",
      "40. ROI_179 重要性分数: 0.2005\n",
      "41. ROI_157 重要性分数: 0.2003\n",
      "42. ROI_156 重要性分数: 0.2002\n",
      "43. ROI_062 重要性分数: 0.1999\n",
      "44. ROI_186 重要性分数: 0.1996\n",
      "45. ROI_012 重要性分数: 0.1995\n",
      "46. ROI_144 重要性分数: 0.1995\n",
      "47. ROI_135 重要性分数: 0.1994\n",
      "48. ROI_065 重要性分数: 0.1993\n",
      "49. ROI_020 重要性分数: 0.1987\n",
      "50. ROI_192 重要性分数: 0.1982\n",
      "51. ROI_069 重要性分数: 0.1981\n",
      "52. ROI_037 重要性分数: 0.1976\n",
      "53. ROI_088 重要性分数: 0.1972\n",
      "54. ROI_024 重要性分数: 0.1970\n",
      "55. ROI_027 重要性分数: 0.1969\n",
      "56. ROI_163 重要性分数: 0.1968\n",
      "57. ROI_133 重要性分数: 0.1968\n",
      "58. ROI_006 重要性分数: 0.1968\n",
      "59. ROI_018 重要性分数: 0.1967\n",
      "60. ROI_128 重要性分数: 0.1966\n",
      "61. ROI_087 重要性分数: 0.1962\n",
      "62. ROI_154 重要性分数: 0.1961\n",
      "63. ROI_085 重要性分数: 0.1959\n",
      "64. ROI_110 重要性分数: 0.1958\n",
      "65. ROI_053 重要性分数: 0.1957\n",
      "66. ROI_113 重要性分数: 0.1954\n",
      "67. ROI_172 重要性分数: 0.1954\n",
      "68. ROI_126 重要性分数: 0.1949\n",
      "69. ROI_164 重要性分数: 0.1948\n",
      "70. ROI_092 重要性分数: 0.1948\n",
      "71. ROI_015 重要性分数: 0.1947\n",
      "72. ROI_040 重要性分数: 0.1947\n",
      "73. ROI_122 重要性分数: 0.1946\n",
      "74. ROI_152 重要性分数: 0.1944\n",
      "75. ROI_165 重要性分数: 0.1941\n",
      "76. ROI_072 重要性分数: 0.1941\n",
      "77. ROI_193 重要性分数: 0.1939\n",
      "78. ROI_108 重要性分数: 0.1937\n",
      "79. ROI_044 重要性分数: 0.1936\n",
      "80. ROI_093 重要性分数: 0.1936\n",
      "81. ROI_042 重要性分数: 0.1936\n",
      "82. ROI_174 重要性分数: 0.1935\n",
      "83. ROI_195 重要性分数: 0.1935\n",
      "84. ROI_132 重要性分数: 0.1934\n",
      "85. ROI_008 重要性分数: 0.1933\n",
      "86. ROI_160 重要性分数: 0.1931\n",
      "87. ROI_139 重要性分数: 0.1931\n",
      "88. ROI_116 重要性分数: 0.1930\n",
      "89. ROI_081 重要性分数: 0.1929\n",
      "90. ROI_078 重要性分数: 0.1927\n",
      "91. ROI_125 重要性分数: 0.1925\n",
      "92. ROI_105 重要性分数: 0.1924\n",
      "93. ROI_178 重要性分数: 0.1924\n",
      "94. ROI_187 重要性分数: 0.1923\n",
      "95. ROI_197 重要性分数: 0.1918\n",
      "96. ROI_112 重要性分数: 0.1916\n",
      "97. ROI_176 重要性分数: 0.1916\n",
      "98. ROI_191 重要性分数: 0.1913\n",
      "99. ROI_118 重要性分数: 0.1913\n",
      "100. ROI_115 重要性分数: 0.1912\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 366\u001b[0m\n\u001b[1;32m    363\u001b[0m fmri_test_loader \u001b[38;5;241m=\u001b[39m fmri_loader\u001b[38;5;241m.\u001b[39mget_dataloaders()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# 运行分析\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m importance_scores, roi_names \u001b[38;5;241m=\u001b[39m get_top_rois(\n\u001b[1;32m    367\u001b[0m     model\u001b[38;5;241m=\u001b[39mgnn_model,\n\u001b[1;32m    368\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mfmri_train_loader,\n\u001b[1;32m    369\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    370\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    371\u001b[0m     roi_names\u001b[38;5;241m=\u001b[39mcc200_names\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''if importance_scores is not None:\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m    import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    plt.ylabel(\"Importance Score\")\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    plt.show()'''\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as tg_nn\n",
    "import h5py\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graph_type = \"cc200\"\n",
    "\n",
    "class fMRIDataLoader:\n",
    "    def __init__(self, file_path, graph_type, test_size=0.15, val_size=0.15, batch_size=32):\n",
    "        self.file_path = file_path\n",
    "        self.graph_type = graph_type\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.batch_size = batch_size\n",
    "        self.data_splits = self._load_and_split_data()\n",
    "    \n",
    "    def _load_and_split_data(self):\n",
    "        \n",
    "        graph_dataset = []\n",
    "        labels = []\n",
    "        with h5py.File(self.file_path, \"r\") as f:\n",
    "            patients_group = f[\"/patients\"]\n",
    "            for subject_id in patients_group.keys():\n",
    "                subject_group = patients_group[subject_id]\n",
    "                if self.graph_type in subject_group and \"y\" in subject_group.attrs:\n",
    "                    triu_vector = subject_group[self.graph_type][:]\n",
    "                    matrix = reconstruct_fc(triu_vector)\n",
    "                    # 生成40000维输入特征（展平整个矩阵）\n",
    "                    node_features = matrix.flatten()  # 形状 (200x200=40000,)\n",
    "                    edge_index = self._get_brain_connectivity_edges(matrix)\n",
    "                    try:\n",
    "                        label_value = subject_group.attrs[\"y\"]\n",
    "                        if isinstance(label_value, (int, np.number)):\n",
    "                            graph_data = Data(\n",
    "                                x=torch.FloatTensor(node_features).view(1, -1),  # 直接使用展平后的矩阵\n",
    "                                edge_index=edge_index,\n",
    "                                y=torch.tensor([label_value], dtype=torch.long)\n",
    "                            )\n",
    "                            graph_dataset.append(graph_data)\n",
    "                            labels.append(label_value)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"Warning: Subject {subject_id} missing {self.graph_type} or label.\")\n",
    "        \n",
    "        # Data splitting logic remains the same\n",
    "        train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "            graph_dataset, labels, test_size=self.test_size, random_state=42\n",
    "        )\n",
    "        train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "            train_val_data, train_val_labels, test_size=self.val_size/(1-self.test_size), random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"train\": (train_data, train_labels),\n",
    "            \"valid\": (val_data, val_labels),\n",
    "            \"test\": (test_data, test_labels),\n",
    "        }\n",
    "    \n",
    "    def _get_brain_connectivity_edges(self, matrix, threshold=0.3):\n",
    "        \"\"\"生成边索引（优化版本）\"\"\"\n",
    "        # 创建全连接（考虑对称性）\n",
    "        rows, cols = np.triu_indices_from(matrix, k=1)\n",
    "        mask = matrix[rows, cols] > threshold\n",
    "        edge_index = np.array([rows[mask], cols[mask]])\n",
    "        \n",
    "        # 添加反向边\n",
    "        edge_index = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
    "        \n",
    "        return torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "    def _create_dataloader(self, data, labels):\n",
    "        return DataLoader(\n",
    "            data, \n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    '''def _create_dataloader(self, data, labels):\n",
    "        return DataLoader(\n",
    "            data, \n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda batch: (\n",
    "                torch.stack([d.x.view(1, 200, 2) for d in batch]),  # 处理特征\n",
    "                torch.cat([d.y for d in batch])  # 正确拼接1维标签\n",
    "            )\n",
    "        )'''\n",
    "    \n",
    "    def get_dataloaders(self):\n",
    "        return {\n",
    "            \"train\": self._create_dataloader(*self.data_splits[\"train\"]),\n",
    "            \"valid\": self._create_dataloader(*self.data_splits[\"valid\"]),\n",
    "            \"test\": self._create_dataloader(*self.data_splits[\"test\"]),\n",
    "        }\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return len(set(self.data_splits[\"train\"][1]))\n",
    "\n",
    "class fMRI3DGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.graph_builder = nn.Sequential(\n",
    "            nn.Linear(40000, 200*200),  # 将40000维输入转换为200x200矩阵\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # 修改后的卷积层定义\n",
    "        self.convs = nn.ModuleList([\n",
    "            # 第一层GAT：输入特征维度需与增强后的节点特征匹配\n",
    "            tg_nn.GATv2Conv(\n",
    "                in_channels=16,  # 修改为特征增强后的维度\n",
    "                out_channels=128,\n",
    "                heads=8,\n",
    "                dropout=config['dropout'],\n",
    "                add_self_loops=False\n",
    "            ),\n",
    "            # 第二层GAT\n",
    "            tg_nn.GATv2Conv(\n",
    "                in_channels=128*8,  # 多头注意力的输出维度\n",
    "                out_channels=256,\n",
    "                heads=4,\n",
    "                dropout=config['dropout']\n",
    "            ),\n",
    "            # 第三层GAT\n",
    "            tg_nn.GATv2Conv(\n",
    "                in_channels=256*4,\n",
    "                out_channels=512,\n",
    "                heads=1,\n",
    "                dropout=config['dropout']\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # 新增特征增强层\n",
    "        self.feature_enhancer = nn.Sequential(\n",
    "            nn.Linear(2, 8),  # 扩展节点特征维度\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.LayerNorm(16)\n",
    "        )\n",
    "\n",
    "        # 更新分类器输入维度\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, config['num_classes'])\n",
    "        )\n",
    "    def build_graph(self, fc_matrix):\n",
    "        \"\"\"修正后的图构建方法\"\"\"\n",
    "        batch_size = fc_matrix.size(0)\n",
    "        \n",
    "        # 通过graph_builder生成邻接矩阵\n",
    "        adj = self.graph_builder(fc_matrix).view(batch_size, 200, 200).float()\n",
    "        adj = (adj + adj.transpose(1,2)) / 2  # 确保对称性\n",
    "\n",
    "        # 生成增强节点特征（维度验证）\n",
    "        node_features = []\n",
    "        edge_indices = []\n",
    "        for b in range(batch_size):\n",
    "            # 基础统计特征\n",
    "            \n",
    "            means = adj[b].mean(dim=1, keepdim=True)  # (200,1)\n",
    "            stds = adj[b].std(dim=1, keepdim=True)    # (200,1)\n",
    "            base_feat = torch.cat([means, stds], dim=1)  # (200,2)\n",
    "            \n",
    "            # 特征增强（输出维度16）\n",
    "            enhanced_feat = self.feature_enhancer(base_feat)  # (200,16)\n",
    "            \n",
    "            # 动态边生成（带阈值限制）\n",
    "            assert adj[b].dtype == torch.float32, f\"邻接矩阵数据类型错误: {adj[b].dtype}\"\n",
    "            threshold = torch.quantile(adj[b].flatten(), 0.75)\n",
    "            mask = (adj[b] > threshold).float()\n",
    "            row, col = mask.nonzero(as_tuple=False).t()\n",
    "            edge_index = torch.stack([row, col], dim=0)\n",
    "            \n",
    "            node_features.append(enhanced_feat)\n",
    "            edge_indices.append(edge_index)\n",
    "\n",
    "        return Batch.from_data_list([\n",
    "            Data(x=feat, edge_index=edge) \n",
    "            for feat, edge in zip(node_features, edge_indices)\n",
    "        ])\n",
    "    \n",
    "\n",
    "    def _generate_adaptive_edges(self, node_feat):\n",
    "        \"\"\"动态生成边连接\"\"\"\n",
    "        # 空间约束\n",
    "        spatial_dist = torch.cdist(self.spatial_emb.weight, self.spatial_emb.weight)\n",
    "        \n",
    "        # 特征相似性\n",
    "        feat_sim = torch.mm(node_feat, node_feat.t())\n",
    "        \n",
    "        # 综合边权重\n",
    "        combined = (feat_sim * (1 / (spatial_dist + 1e-6)))\n",
    "        \n",
    "        # 生成邻接矩阵\n",
    "        adj = (combined > self.threshold).float()\n",
    "        \n",
    "        # 确保最小连接数\n",
    "        topk = torch.topk(combined, self.k_neighbors, dim=1)\n",
    "        adj[topk.indices] = 1.0\n",
    "        \n",
    "        return adj\n",
    "    def forward(self, raw_fc):\n",
    "        # 输入应为 [batch_size, 40000]\n",
    "        if raw_fc.dim() == 1:\n",
    "            raw_fc = raw_fc.unsqueeze(0)  # 添加批次维度 [1, 40000]\n",
    "        assert raw_fc.dim() == 2, f\"输入应为二维张量 [batch, 40000]，当前维度：{raw_fc.dim()}\"\n",
    "        batch_size = raw_fc.size(0)\n",
    "        \n",
    "        # 构建动态图\n",
    "        batch_graph = self.build_graph(raw_fc)\n",
    "        \n",
    "        # 图卷积处理\n",
    "        x = batch_graph.x  # [batch*200, 16] (经过特征增强)\n",
    "        edge_index = batch_graph.edge_index\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # 全局池化\n",
    "        x = tg_nn.global_mean_pool(x, batch_graph.batch)  # [batch, hidden_dim]\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def _adjust_model_parameters(self, new_dim):\n",
    "        \"\"\"动态调整模型参数\"\"\"\n",
    "        # 调整空间嵌入维度\n",
    "        old_emb = self.spatial_emb\n",
    "        self.spatial_emb = nn.Embedding(new_dim, 3)\n",
    "        with torch.no_grad():\n",
    "            min_dim = min(old_emb.weight.size(0), new_dim)\n",
    "            self.spatial_emb.weight[:min_dim] = old_emb.weight[:min_dim]\n",
    "        \n",
    "        # 调整图卷积层输入维度\n",
    "        if self.convs[0].in_channels != new_dim:\n",
    "            first_conv = self.convs[0]\n",
    "            new_conv = tg_nn.GATv2Conv(\n",
    "                new_dim, \n",
    "                first_conv.out_channels,\n",
    "                heads=first_conv.heads,\n",
    "                dropout=first_conv.dropout\n",
    "            )\n",
    "            self.convs[0] = new_conv\n",
    "\n",
    "def load_pretrained_gnn():\n",
    "    GNN_CONFIG = {\n",
    "        \"gnn_layers\": 3,\n",
    "        \"hidden_channels\": 128,\n",
    "        \"num_classes\": 2,\n",
    "        \"dropout\": 0.4\n",
    "    }\n",
    "    model = fMRI3DGNN(GNN_CONFIG)\n",
    "    checkpoint = torch.load(\"fmri_gnn.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model.to(device)\n",
    "\n",
    "def reconstruct_fc(vector):\n",
    "    \"\"\"将上三角向量重建为对称矩阵\"\"\"\n",
    "    # 创建空矩阵\n",
    "    matrix = np.zeros((200, 200))\n",
    "    # 提取上三角索引（不包括对角线）\n",
    "    triu_indices = np.triu_indices(200, k=1)\n",
    "    # 填充上三角\n",
    "    matrix[triu_indices] = vector\n",
    "    # 对称复制到下三角\n",
    "    matrix = matrix + matrix.T - np.diag(matrix.diagonal())    \n",
    "    return matrix\n",
    "\n",
    "def get_top_rois(model, dataloader, device, top_k=100, roi_names=None):\n",
    "    model.eval()\n",
    "    gradients = []\n",
    "    \n",
    "    progress = tqdm(dataloader, desc=\"计算ROI重要性\", unit=\"batch\")\n",
    "    \n",
    "    try:\n",
    "        for batch in progress:\n",
    "            # 解包批次数据为 (input_features, labels)\n",
    "            x_batch = batch.x.to(device)\n",
    "            print(\"输入维度检查:\", x_batch.shape) # [batch_size, 40000]\n",
    "            x_batch = x_batch.requires_grad_(True)  # [batch_size, 40000]\n",
    "            \n",
    "            y_batch = batch.y  # [batch_size]\n",
    "            \n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(x_batch)  # 确保输入是 [batch_size, 40000]\n",
    "            \n",
    "            # 梯度计算\n",
    "            grads = torch.autograd.grad(\n",
    "                outputs.sum(),\n",
    "                x_batch,\n",
    "                retain_graph=False,\n",
    "                create_graph=False,\n",
    "                allow_unused=True\n",
    "            )\n",
    "            \n",
    "            # 记录梯度\n",
    "            gradients.append(grads[0].abs().mean(dim=0).cpu())\n",
    " \n",
    "        # 合并梯度\n",
    "        all_grads = torch.stack(gradients).mean(dim=0).numpy()\n",
    "        \n",
    "        # 转换为连接矩阵\n",
    "        conn_matrix = all_grads.reshape(200, 200)\n",
    "        roi_importance = conn_matrix.sum(0) + conn_matrix.sum(1)\n",
    "        \n",
    "        # 结果处理\n",
    "        sorted_indices = np.argsort(roi_importance)[::-1]\n",
    "        roi_names = roi_names or [f\"ROI_{i+1:03d}\" for i in range(200)]\n",
    "        \n",
    "        '''print(f\"\\nTop {top_k} fMRI ROIs:\")\n",
    "        for i in range(top_k):\n",
    "            idx = sorted_indices[i]\n",
    "            print(f\"{i+1}. {roi_names[idx]} ({roi_importance[idx]:.4f})\")\n",
    "            \n",
    "        return roi_importance, roi_names'''\n",
    "        print(f\"\\nTop {top_k} fMRI ROIs (按索引显示):\")\n",
    "        for i in range(top_k):\n",
    "            idx = sorted_indices[i]\n",
    "            print(f\"{i+1}. ROI_{idx+1:03d} 重要性分数: {roi_importance[idx]:.4f}\")\n",
    "            \n",
    "        return roi_importance\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"分析失败: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# 使用示例 --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载预训练模型\n",
    "    gnn_model = load_pretrained_gnn().to(device)\n",
    "    file_path = \"/home/yangzongxian/xlz/ASD_GCN/main/data2/abide.hdf5\"\n",
    "    graph_type = \"cc200\"\n",
    "    fmri_loader = fMRIDataLoader(file_path = file_path, graph_type=\"cc200\", batch_size=32)\n",
    "    cc200_names = [\n",
    "        \"Precentral_L\", \"Precentral_R\",  # 实际应使用完整的200个名称\n",
    "        # ... 补充完整名称列表\n",
    "    ]\n",
    "    \n",
    "    # 获取测试集数据加载器\n",
    "    fmri_train_loader = fmri_loader.get_dataloaders()[\"train\"]\n",
    "    fmri_val_loader = fmri_loader.get_dataloaders()[\"valid\"]\n",
    "    fmri_test_loader = fmri_loader.get_dataloaders()[\"test\"]\n",
    "    \n",
    "    # 运行分析\n",
    "    importance_scores, roi_names = get_top_rois(\n",
    "        model=gnn_model,\n",
    "        dataloader=fmri_train_loader,\n",
    "        device=device,\n",
    "        top_k=100,\n",
    "        roi_names=cc200_names\n",
    "    )\n",
    "    \n",
    "    \n",
    "    '''if importance_scores is not None:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(len(importance_scores)), importance_scores)\n",
    "        plt.title(\"ROI Importance Distribution\")\n",
    "        plt.xlabel(\"ROI Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_rois_signed(model, dataloader, device, top_k=100, roi_names=None):\n",
    "    model.eval()\n",
    "    gradients = []\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch.x.to(device).requires_grad_(True)\n",
    "        y_batch = batch.y.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        # 假设二分类，计算ASD类（假设为1）的梯度\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs[:, 1].sum(),  # ASD类的概率和\n",
    "            x_batch,\n",
    "            retain_graph=False,\n",
    "            create_graph=False\n",
    "        )[0]\n",
    "        gradients.append(grads.mean(dim=0).cpu())  # 批次平均，符号保留\n",
    "    all_grads = torch.stack(gradients).mean(dim=0).numpy()\n",
    "    conn_matrix = all_grads.reshape(200, 200)\n",
    "    roi_importance = conn_matrix.sum(0) + conn_matrix.sum(1)  # 每个ROI的连接梯度和\n",
    "    sorted_indices = np.argsort(np.abs(roi_importance))[::-1]\n",
    "    roi_names = roi_names or [f\"ROI_{i+1:03d}\" for i in range(200)]\n",
    "    print(f\"\\nTop {top_k} fMRI ROIs (signed importance):\")\n",
    "    for i in range(top_k):\n",
    "        idx = sorted_indices[i]\n",
    "        score = roi_importance[idx]\n",
    "        direction = \"高连接性\" if score > 0 else \"低连接性\"\n",
    "        print(f\"{i+1}. {roi_names[idx]} ({score:.4f}, {direction} in ASD)\")\n",
    "    return roi_importance, roi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2848027/3685911741.py:273: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"fmri_gnn.pth\", map_location=device)\n",
      "/home/yangzongxian/anaconda3/envs/xlz1/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 100 fMRI ROIs (signed importance):\n",
      "1. ROI_078 (-0.5556, 低连接性 in ASD)\n",
      "2. ROI_077 (-0.5048, 低连接性 in ASD)\n",
      "3. ROI_070 (-0.4873, 低连接性 in ASD)\n",
      "4. ROI_003 (0.4195, 高连接性 in ASD)\n",
      "5. ROI_038 (0.4012, 高连接性 in ASD)\n",
      "6. ROI_105 (-0.3547, 低连接性 in ASD)\n",
      "7. ROI_074 (0.3320, 高连接性 in ASD)\n",
      "8. ROI_056 (0.3180, 高连接性 in ASD)\n",
      "9. ROI_018 (0.3104, 高连接性 in ASD)\n",
      "10. ROI_112 (-0.3073, 低连接性 in ASD)\n",
      "11. ROI_151 (-0.3042, 低连接性 in ASD)\n",
      "12. ROI_164 (-0.2995, 低连接性 in ASD)\n",
      "13. ROI_076 (-0.2982, 低连接性 in ASD)\n",
      "14. ROI_021 (-0.2939, 低连接性 in ASD)\n",
      "15. ROI_023 (0.2882, 高连接性 in ASD)\n",
      "16. ROI_122 (0.2860, 高连接性 in ASD)\n",
      "17. ROI_050 (0.2818, 高连接性 in ASD)\n",
      "18. ROI_120 (0.2785, 高连接性 in ASD)\n",
      "19. ROI_194 (-0.2748, 低连接性 in ASD)\n",
      "20. ROI_093 (0.2680, 高连接性 in ASD)\n",
      "21. ROI_035 (0.2679, 高连接性 in ASD)\n",
      "22. ROI_099 (0.2661, 高连接性 in ASD)\n",
      "23. ROI_107 (-0.2601, 低连接性 in ASD)\n",
      "24. ROI_072 (-0.2585, 低连接性 in ASD)\n",
      "25. ROI_073 (0.2555, 高连接性 in ASD)\n",
      "26. ROI_171 (0.2537, 高连接性 in ASD)\n",
      "27. ROI_106 (-0.2531, 低连接性 in ASD)\n",
      "28. ROI_055 (-0.2479, 低连接性 in ASD)\n",
      "29. ROI_051 (0.2417, 高连接性 in ASD)\n",
      "30. ROI_019 (0.2388, 高连接性 in ASD)\n",
      "31. ROI_148 (-0.2364, 低连接性 in ASD)\n",
      "32. ROI_165 (0.2349, 高连接性 in ASD)\n",
      "33. ROI_196 (-0.2344, 低连接性 in ASD)\n",
      "34. ROI_103 (-0.2343, 低连接性 in ASD)\n",
      "35. ROI_022 (0.2340, 高连接性 in ASD)\n",
      "36. ROI_026 (-0.2327, 低连接性 in ASD)\n",
      "37. ROI_160 (-0.2297, 低连接性 in ASD)\n",
      "38. ROI_177 (0.2267, 高连接性 in ASD)\n",
      "39. ROI_040 (-0.2162, 低连接性 in ASD)\n",
      "40. ROI_075 (-0.2160, 低连接性 in ASD)\n",
      "41. ROI_025 (-0.2146, 低连接性 in ASD)\n",
      "42. ROI_027 (0.2076, 高连接性 in ASD)\n",
      "43. ROI_118 (0.2048, 高连接性 in ASD)\n",
      "44. ROI_054 (0.2044, 高连接性 in ASD)\n",
      "45. ROI_128 (0.2018, 高连接性 in ASD)\n",
      "46. ROI_127 (0.1997, 高连接性 in ASD)\n",
      "47. ROI_004 (0.1977, 高连接性 in ASD)\n",
      "48. ROI_091 (-0.1950, 低连接性 in ASD)\n",
      "49. ROI_083 (0.1916, 高连接性 in ASD)\n",
      "50. ROI_030 (-0.1909, 低连接性 in ASD)\n",
      "51. ROI_047 (0.1902, 高连接性 in ASD)\n",
      "52. ROI_064 (0.1869, 高连接性 in ASD)\n",
      "53. ROI_156 (0.1854, 高连接性 in ASD)\n",
      "54. ROI_063 (0.1835, 高连接性 in ASD)\n",
      "55. ROI_154 (-0.1815, 低连接性 in ASD)\n",
      "56. ROI_036 (-0.1769, 低连接性 in ASD)\n",
      "57. ROI_069 (-0.1755, 低连接性 in ASD)\n",
      "58. ROI_199 (-0.1735, 低连接性 in ASD)\n",
      "59. ROI_060 (0.1731, 高连接性 in ASD)\n",
      "60. ROI_181 (-0.1681, 低连接性 in ASD)\n",
      "61. ROI_048 (-0.1676, 低连接性 in ASD)\n",
      "62. ROI_155 (0.1667, 高连接性 in ASD)\n",
      "63. ROI_092 (-0.1651, 低连接性 in ASD)\n",
      "64. ROI_085 (0.1650, 高连接性 in ASD)\n",
      "65. ROI_007 (0.1626, 高连接性 in ASD)\n",
      "66. ROI_142 (-0.1613, 低连接性 in ASD)\n",
      "67. ROI_133 (-0.1610, 低连接性 in ASD)\n",
      "68. ROI_182 (-0.1604, 低连接性 in ASD)\n",
      "69. ROI_089 (-0.1600, 低连接性 in ASD)\n",
      "70. ROI_068 (-0.1591, 低连接性 in ASD)\n",
      "71. ROI_146 (-0.1567, 低连接性 in ASD)\n",
      "72. ROI_049 (-0.1563, 低连接性 in ASD)\n",
      "73. ROI_124 (-0.1559, 低连接性 in ASD)\n",
      "74. ROI_163 (0.1558, 高连接性 in ASD)\n",
      "75. ROI_011 (-0.1540, 低连接性 in ASD)\n",
      "76. ROI_067 (0.1505, 高连接性 in ASD)\n",
      "77. ROI_115 (-0.1499, 低连接性 in ASD)\n",
      "78. ROI_100 (-0.1493, 低连接性 in ASD)\n",
      "79. ROI_178 (-0.1471, 低连接性 in ASD)\n",
      "80. ROI_131 (-0.1470, 低连接性 in ASD)\n",
      "81. ROI_061 (0.1464, 高连接性 in ASD)\n",
      "82. ROI_088 (0.1456, 高连接性 in ASD)\n",
      "83. ROI_137 (0.1444, 高连接性 in ASD)\n",
      "84. ROI_116 (0.1410, 高连接性 in ASD)\n",
      "85. ROI_015 (0.1400, 高连接性 in ASD)\n",
      "86. ROI_109 (-0.1393, 低连接性 in ASD)\n",
      "87. ROI_143 (-0.1337, 低连接性 in ASD)\n",
      "88. ROI_195 (-0.1324, 低连接性 in ASD)\n",
      "89. ROI_119 (-0.1284, 低连接性 in ASD)\n",
      "90. ROI_172 (-0.1280, 低连接性 in ASD)\n",
      "91. ROI_139 (0.1276, 高连接性 in ASD)\n",
      "92. ROI_042 (-0.1276, 低连接性 in ASD)\n",
      "93. ROI_159 (0.1274, 高连接性 in ASD)\n",
      "94. ROI_053 (0.1266, 高连接性 in ASD)\n",
      "95. ROI_094 (-0.1265, 低连接性 in ASD)\n",
      "96. ROI_175 (-0.1230, 低连接性 in ASD)\n",
      "97. ROI_134 (0.1223, 高连接性 in ASD)\n",
      "98. ROI_130 (0.1202, 高连接性 in ASD)\n",
      "99. ROI_113 (-0.1198, 低连接性 in ASD)\n",
      "100. ROI_188 (0.1189, 高连接性 in ASD)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.11101106e-01,  8.69523734e-02,  4.19450223e-01,  1.97669297e-01,\n",
       "        -2.72064507e-02, -1.10024229e-01,  1.62590012e-01, -3.79957855e-02,\n",
       "        -6.04615994e-02, -6.33042585e-03, -1.54018074e-01,  1.16986372e-01,\n",
       "        -1.08430833e-02,  9.13844705e-02,  1.39984697e-01,  7.01157898e-02,\n",
       "         5.66040464e-02,  3.10375750e-01,  2.38823235e-01, -9.14340168e-02,\n",
       "        -2.93935835e-01,  2.33978152e-01,  2.88170934e-01,  8.11973512e-02,\n",
       "        -2.14589223e-01, -2.32651979e-01,  2.07617521e-01, -2.72190943e-02,\n",
       "         4.89774086e-02, -1.90916628e-01,  6.58501536e-02,  2.07267888e-02,\n",
       "        -1.80101488e-04,  2.03146115e-02,  2.67891377e-01, -1.76919878e-01,\n",
       "         5.34295961e-02,  4.01191175e-01, -9.18172300e-02, -2.16218352e-01,\n",
       "         2.39921845e-02, -1.27599031e-01, -1.15855806e-01, -6.44879490e-02,\n",
       "         2.99030561e-02, -4.85641323e-02,  1.90229088e-01, -1.67647377e-01,\n",
       "        -1.56337559e-01,  2.81822443e-01,  2.41671354e-01,  8.97357762e-02,\n",
       "         1.26635969e-01,  2.04415321e-01, -2.47891873e-01,  3.18016648e-01,\n",
       "         6.05787598e-02,  1.08333081e-01, -3.84061337e-02,  1.73117906e-01,\n",
       "         1.46448612e-01, -5.55745838e-03,  1.83498621e-01,  1.86859667e-01,\n",
       "         1.07870236e-01, -5.42566217e-02,  1.50520295e-01, -1.59129530e-01,\n",
       "        -1.75527051e-01, -4.87263858e-01, -4.04585749e-02, -2.58490682e-01,\n",
       "         2.55479991e-01,  3.32036883e-01, -2.16046527e-01, -2.98238933e-01,\n",
       "        -5.04849434e-01, -5.55590630e-01, -5.63802645e-02, -8.47922545e-03,\n",
       "        -9.15141851e-02, -1.20389611e-02,  1.91633254e-01,  5.87862879e-02,\n",
       "         1.64978623e-01, -6.34935945e-02,  9.68176499e-02,  1.45580232e-01,\n",
       "        -1.59981310e-01, -5.94069287e-02, -1.95016772e-01, -1.65101916e-01,\n",
       "         2.68013954e-01, -1.26500249e-01,  1.13781556e-01, -9.05598253e-02,\n",
       "        -1.07444212e-01,  6.15970641e-02,  2.66147614e-01, -1.49308801e-01,\n",
       "         7.70800412e-02,  8.31152052e-02, -2.34315321e-01, -3.19889225e-02,\n",
       "        -3.54706466e-01, -2.53145635e-01, -2.60086507e-01,  3.62467468e-02,\n",
       "        -1.39301568e-01, -2.50345282e-02, -2.58854255e-02, -3.07280183e-01,\n",
       "        -1.19758204e-01, -4.75727841e-02, -1.49934173e-01,  1.41028225e-01,\n",
       "         5.85433729e-02,  2.04771489e-01, -1.28363431e-01,  2.78508008e-01,\n",
       "        -1.15037352e-01,  2.86022991e-01, -3.40833850e-02, -1.55903384e-01,\n",
       "         2.95968959e-03,  2.95712389e-02,  1.99701071e-01,  2.01761574e-01,\n",
       "        -4.06308845e-02,  1.20157972e-01, -1.47005439e-01,  1.02145039e-02,\n",
       "        -1.60955459e-01,  1.22269973e-01, -4.06114310e-02,  2.96035595e-02,\n",
       "         1.44449055e-01, -7.22914338e-02,  1.27625734e-01,  1.03619415e-02,\n",
       "        -7.35731646e-02, -1.61343247e-01, -1.33709952e-01, -8.10575560e-02,\n",
       "         8.82854164e-02, -1.56662256e-01, -4.74017784e-02, -2.36404464e-01,\n",
       "        -1.16663754e-01, -7.63817737e-03, -3.04205000e-01, -1.08725607e-01,\n",
       "        -1.16478406e-01, -1.81456655e-01,  1.66733950e-01,  1.85410678e-01,\n",
       "         8.29937868e-03,  4.73882779e-02,  1.27401903e-01, -2.29685605e-01,\n",
       "         5.48675284e-02,  1.35873863e-02,  1.55768037e-01, -2.99472272e-01,\n",
       "         2.34877899e-01, -2.20953859e-02, -8.76209140e-02,  8.35499316e-02,\n",
       "        -4.65962216e-02,  3.00267003e-02,  2.53716350e-01, -1.28010675e-01,\n",
       "         2.17752391e-03, -8.92587379e-02, -1.23030916e-01, -3.83813232e-02,\n",
       "         2.26705372e-01, -1.47126302e-01, -1.90017074e-02, -8.55943859e-02,\n",
       "        -1.68053120e-01, -1.60357505e-01, -7.70134777e-02, -2.55033001e-02,\n",
       "         1.78892538e-03, -1.00539461e-01,  1.07664987e-01,  1.18877992e-01,\n",
       "        -4.23712991e-02,  1.14877507e-01, -2.83085126e-02, -4.96951006e-02,\n",
       "        -1.94021240e-02, -2.74828017e-01, -1.32445365e-01, -2.34355956e-01,\n",
       "        -2.32630260e-02, -6.37485608e-02, -1.73466146e-01,  1.18852004e-01],\n",
       "       dtype=float32),\n",
       " ['ROI_001',\n",
       "  'ROI_002',\n",
       "  'ROI_003',\n",
       "  'ROI_004',\n",
       "  'ROI_005',\n",
       "  'ROI_006',\n",
       "  'ROI_007',\n",
       "  'ROI_008',\n",
       "  'ROI_009',\n",
       "  'ROI_010',\n",
       "  'ROI_011',\n",
       "  'ROI_012',\n",
       "  'ROI_013',\n",
       "  'ROI_014',\n",
       "  'ROI_015',\n",
       "  'ROI_016',\n",
       "  'ROI_017',\n",
       "  'ROI_018',\n",
       "  'ROI_019',\n",
       "  'ROI_020',\n",
       "  'ROI_021',\n",
       "  'ROI_022',\n",
       "  'ROI_023',\n",
       "  'ROI_024',\n",
       "  'ROI_025',\n",
       "  'ROI_026',\n",
       "  'ROI_027',\n",
       "  'ROI_028',\n",
       "  'ROI_029',\n",
       "  'ROI_030',\n",
       "  'ROI_031',\n",
       "  'ROI_032',\n",
       "  'ROI_033',\n",
       "  'ROI_034',\n",
       "  'ROI_035',\n",
       "  'ROI_036',\n",
       "  'ROI_037',\n",
       "  'ROI_038',\n",
       "  'ROI_039',\n",
       "  'ROI_040',\n",
       "  'ROI_041',\n",
       "  'ROI_042',\n",
       "  'ROI_043',\n",
       "  'ROI_044',\n",
       "  'ROI_045',\n",
       "  'ROI_046',\n",
       "  'ROI_047',\n",
       "  'ROI_048',\n",
       "  'ROI_049',\n",
       "  'ROI_050',\n",
       "  'ROI_051',\n",
       "  'ROI_052',\n",
       "  'ROI_053',\n",
       "  'ROI_054',\n",
       "  'ROI_055',\n",
       "  'ROI_056',\n",
       "  'ROI_057',\n",
       "  'ROI_058',\n",
       "  'ROI_059',\n",
       "  'ROI_060',\n",
       "  'ROI_061',\n",
       "  'ROI_062',\n",
       "  'ROI_063',\n",
       "  'ROI_064',\n",
       "  'ROI_065',\n",
       "  'ROI_066',\n",
       "  'ROI_067',\n",
       "  'ROI_068',\n",
       "  'ROI_069',\n",
       "  'ROI_070',\n",
       "  'ROI_071',\n",
       "  'ROI_072',\n",
       "  'ROI_073',\n",
       "  'ROI_074',\n",
       "  'ROI_075',\n",
       "  'ROI_076',\n",
       "  'ROI_077',\n",
       "  'ROI_078',\n",
       "  'ROI_079',\n",
       "  'ROI_080',\n",
       "  'ROI_081',\n",
       "  'ROI_082',\n",
       "  'ROI_083',\n",
       "  'ROI_084',\n",
       "  'ROI_085',\n",
       "  'ROI_086',\n",
       "  'ROI_087',\n",
       "  'ROI_088',\n",
       "  'ROI_089',\n",
       "  'ROI_090',\n",
       "  'ROI_091',\n",
       "  'ROI_092',\n",
       "  'ROI_093',\n",
       "  'ROI_094',\n",
       "  'ROI_095',\n",
       "  'ROI_096',\n",
       "  'ROI_097',\n",
       "  'ROI_098',\n",
       "  'ROI_099',\n",
       "  'ROI_100',\n",
       "  'ROI_101',\n",
       "  'ROI_102',\n",
       "  'ROI_103',\n",
       "  'ROI_104',\n",
       "  'ROI_105',\n",
       "  'ROI_106',\n",
       "  'ROI_107',\n",
       "  'ROI_108',\n",
       "  'ROI_109',\n",
       "  'ROI_110',\n",
       "  'ROI_111',\n",
       "  'ROI_112',\n",
       "  'ROI_113',\n",
       "  'ROI_114',\n",
       "  'ROI_115',\n",
       "  'ROI_116',\n",
       "  'ROI_117',\n",
       "  'ROI_118',\n",
       "  'ROI_119',\n",
       "  'ROI_120',\n",
       "  'ROI_121',\n",
       "  'ROI_122',\n",
       "  'ROI_123',\n",
       "  'ROI_124',\n",
       "  'ROI_125',\n",
       "  'ROI_126',\n",
       "  'ROI_127',\n",
       "  'ROI_128',\n",
       "  'ROI_129',\n",
       "  'ROI_130',\n",
       "  'ROI_131',\n",
       "  'ROI_132',\n",
       "  'ROI_133',\n",
       "  'ROI_134',\n",
       "  'ROI_135',\n",
       "  'ROI_136',\n",
       "  'ROI_137',\n",
       "  'ROI_138',\n",
       "  'ROI_139',\n",
       "  'ROI_140',\n",
       "  'ROI_141',\n",
       "  'ROI_142',\n",
       "  'ROI_143',\n",
       "  'ROI_144',\n",
       "  'ROI_145',\n",
       "  'ROI_146',\n",
       "  'ROI_147',\n",
       "  'ROI_148',\n",
       "  'ROI_149',\n",
       "  'ROI_150',\n",
       "  'ROI_151',\n",
       "  'ROI_152',\n",
       "  'ROI_153',\n",
       "  'ROI_154',\n",
       "  'ROI_155',\n",
       "  'ROI_156',\n",
       "  'ROI_157',\n",
       "  'ROI_158',\n",
       "  'ROI_159',\n",
       "  'ROI_160',\n",
       "  'ROI_161',\n",
       "  'ROI_162',\n",
       "  'ROI_163',\n",
       "  'ROI_164',\n",
       "  'ROI_165',\n",
       "  'ROI_166',\n",
       "  'ROI_167',\n",
       "  'ROI_168',\n",
       "  'ROI_169',\n",
       "  'ROI_170',\n",
       "  'ROI_171',\n",
       "  'ROI_172',\n",
       "  'ROI_173',\n",
       "  'ROI_174',\n",
       "  'ROI_175',\n",
       "  'ROI_176',\n",
       "  'ROI_177',\n",
       "  'ROI_178',\n",
       "  'ROI_179',\n",
       "  'ROI_180',\n",
       "  'ROI_181',\n",
       "  'ROI_182',\n",
       "  'ROI_183',\n",
       "  'ROI_184',\n",
       "  'ROI_185',\n",
       "  'ROI_186',\n",
       "  'ROI_187',\n",
       "  'ROI_188',\n",
       "  'ROI_189',\n",
       "  'ROI_190',\n",
       "  'ROI_191',\n",
       "  'ROI_192',\n",
       "  'ROI_193',\n",
       "  'ROI_194',\n",
       "  'ROI_195',\n",
       "  'ROI_196',\n",
       "  'ROI_197',\n",
       "  'ROI_198',\n",
       "  'ROI_199',\n",
       "  'ROI_200'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_model = load_pretrained_gnn().to(device)\n",
    "gnn_model = load_pretrained_gnn().to(device)\n",
    "file_path = \"/home/yangzongxian/xlz/ASD_GCN/main/data2/abide.hdf5\"\n",
    "graph_type = \"cc200\"\n",
    "fmri_loader = fMRIDataLoader(file_path = file_path, graph_type=\"cc200\", batch_size=32)\n",
    "fmri_train_loader = fmri_loader.get_dataloaders()[\"train\"]\n",
    "fmri_val_loader = fmri_loader.get_dataloaders()[\"valid\"]\n",
    "fmri_test_loader = fmri_loader.get_dataloaders()[\"test\"]\n",
    "\n",
    "get_top_rois_signed(gnn_model, fmri_train_loader, device, top_k = 100, roi_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "\n",
    "Entrez.email = \"xvlizhao@gmail.com\"\n",
    "\n",
    "brain_regions = [\n",
    "    \"Temporal\", \"Cerebelum\", \"Calcarine\", \"Precuneus\", \"Frontal\",\n",
    "    \"Cingulum\", \"Parietal\", \"Thalamus\", \"Occipital\"\n",
    "]\n",
    "\n",
    "# 清理微生物名称的空格并去重\n",
    "microbes = list(set([m.strip() for m in [\n",
    "    \"Roseburia intestinalis\",\"Roseburia hominis\",\"Parabacteroides chongii\",\"Parabacteroides faecis\",\n",
    "    \"Parabacteroides timonensis\",\"Ruminococcus torques\", \"Mediterraneibacter catenae\",\n",
    "    \"Ruminococcus torques\" ,\"Butyricicoccus pullicaecorum\",\"Butyricicoccus porcorum \",\n",
    "    \"Agathobaculum desmolans\",\"Paraprevotella xylaniphila\", \"Paraprevotella xylaniphila\",\n",
    "    \"Paraprevotella clara\",\"Oribacterium sinus\",\"Oribacterium parvum\",\n",
    "    \"Oribacterium asaccharolyticum\",\"Enterocloster homin\",\"Lacrimispora indolis\",\n",
    "    \"Kineothrix alysoides\",\"Fusicatenibacter saccharivorans\",\"Clostridium porci\",\n",
    "    \"Lacrimispora amygdalina\",\"Intestinimonas butyriciproducens\",\"Intestinimonas timonensis\",\n",
    "    \"Clostridium phoceensis\"\n",
    "]]))\n",
    "\n",
    "def search_asd_studies(region, microbe):\n",
    "    \"\"\"专注搜索ASD领域的三重组合：脑区+微生物+ASD关键词\"\"\"\n",
    "    asd_keywords = '(autism OR ASD OR \"autism spectrum disorder\")'\n",
    "    query = f'({region}[Title/Abstract] AND {microbe}[Title/Abstract]) AND {asd_keywords}'  # 标题/摘要限定\n",
    "    \n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=2, sort=\"relevance\")  # 限制2篇最相关结果\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return record[\"IdList\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching {region} & {microbe}: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_paper_details(id_list):\n",
    "    \"\"\"获取论文详细信息\"\"\"\n",
    "    if not id_list:\n",
    "        return []\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "        records = list(Medline.parse(handle))\n",
    "        handle.close()\n",
    "        return records\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers: {e}\")\n",
    "        return []\n",
    "# 以脑区为主轴遍历\n",
    "for region in brain_regions:\n",
    "    region_save_path = f\"ASD_BrainRegion.txt\"  \n",
    "    \n",
    "    for microbe in microbes:\n",
    "        time.sleep(1.5)  \n",
    "        paper_ids = search_asd_studies(region, microbe)\n",
    "        \n",
    "        if not paper_ids:\n",
    "            continue  \n",
    "        \n",
    "        papers = fetch_paper_details(paper_ids)\n",
    "        with open(region_save_path, 'a') as f:  \n",
    "            f.write(f\"\\n### {region} & {microbe} ###\\n\")\n",
    "            \n",
    "            for paper in papers:\n",
    "                title = paper.get(\"TI\", \"No title\")\n",
    "                pmid = paper.get(\"PMID\", \"\")\n",
    "                url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else \"No URL\"\n",
    "                abstract = paper.get(\"AB\", \"No abstract available\")[:500] + \"...\"  \n",
    "                \n",
    "              \n",
    "                f.write(f\"Region:{region}\\nMicrobe:{microbe}\\nTitle: {title}\\nURL: {url}\\nAbstract: {abstract}\\n\\n\")\n",
    "                print(f\"Found in {region}: {title[:50]}...\")\n",
    "    \n",
    "    print(f\"Completed {region}. Saved to {region_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在处理脑区 (1/9): Temporal\n",
      "\n",
      "正在处理脑区 (2/9): Cerebellum\n",
      "\n",
      "正在处理脑区 (3/9): Calcarine\n",
      "\n",
      "正在处理脑区 (4/9): Precuneus\n",
      "\n",
      "正在处理脑区 (5/9): Frontal\n",
      "\n",
      "正在处理脑区 (6/9): Cingulum\n",
      "\n",
      "正在处理脑区 (7/9): Parietal\n",
      "\n",
      "正在处理脑区 (8/9): Thalamus\n",
      "\n",
      "正在处理脑区 (9/9): Occipital\n",
      "\n",
      "所有结果已保存至: /home/yangzongxian/xlz/ASD_GCN/main/ASD_Brain_Gut_Studies_AllRegions.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "\n",
    "Entrez.email = \"xvlizhao@gmail.com\"\n",
    "\n",
    "# 参数配置\n",
    "SAVE_FILE = \"/home/yangzongxian/xlz/ASD_GCN/main/ASD_Brain_Gut_Studies_AllRegions.txt\"  \n",
    "\n",
    "BRAIN_REGIONS = [\n",
    "    \"Temporal\", \"Cerebellum\", \"Calcarine\", \"Precuneus\", \n",
    "    \"Frontal\", \"Cingulum\", \"Parietal\", \"Thalamus\", \"Occipital\"\n",
    "]\n",
    "MICROBES = [\n",
    "    \"Roseburia intestinalis\",\"Roseburia hominis\",\"Parabacteroides chongii\",\"Parabacteroides faecis\",\n",
    "    \"Parabacteroides timonensis\",\"Ruminococcus torques\", \"Mediterraneibacter catenae\",\n",
    "    \"Ruminococcus torques\" ,\"Butyricicoccus pullicaecorum\",\"Butyricicoccus porcorum \",\n",
    "    \"Agathobaculum desmolans\",\"Paraprevotella xylaniphila\", \"Paraprevotella xylaniphila\",\n",
    "    \"Paraprevotella clara\",\"Oribacterium sinus\",\"Oribacterium parvum\",\n",
    "    \"Oribacterium asaccharolyticum\",\"Enterocloster homin\",\"Lacrimispora indolis\",\n",
    "    \"Kineothrix alysoides\",\"Fusicatenibacter saccharivorans\",\"Clostridium porci\",\n",
    "    \"Lacrimispora amygdalina\",\"Intestinimonas butyriciproducens\",\"Intestinimonas timonensis\",\n",
    "    \"Clostridium phoceensis\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def search_asd_studies(region, microbe):\n",
    "    \"\"\"精准检索ASD相关研究：脑区+微生物+ASD关键词\"\"\"\n",
    "    query = (\n",
    "        f'(\"{region}\"[Title/Abstract] OR \"brain\"[Title/Abstract]) '\n",
    "        f'AND \"{microbe}\"[Title/Abstract] '\n",
    "        'AND (autism OR ASD OR \"autism spectrum disorder\") '\n",
    "        'AND (\"human\"[MeSH Terms] OR \"clinical trial\"[Publication Type])'  # 限制临床研究\n",
    "    )\n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=3, sort=\"relevance\")\n",
    "        results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return results.get(\"IdList\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"检索失败 {region}-{microbe}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def save_results(data):\n",
    "    \"\"\"统一保存结果到文件\"\"\"\n",
    "    with open(SAVE_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "# 初始化输出文件\n",
    "with open(SAVE_FILE, \"w\") as f:\n",
    "    f.write(\"ASD脑-肠轴研究文献汇总\\n\\n\")\n",
    "\n",
    "# 遍历所有脑区与微生物组合\n",
    "for idx, region in enumerate(BRAIN_REGIONS, 1):\n",
    "    print(f\"\\n正在处理脑区 ({idx}/{len(BRAIN_REGIONS)}): {region}\")\n",
    "    \n",
    "    for microbe in MICROBES:\n",
    "        paper_ids = search_asd_studies(region, microbe)\n",
    "        time.sleep(1.2)  # 遵守NCBI API速率限制\n",
    "        \n",
    "        if not paper_ids:\n",
    "            continue\n",
    "        \n",
    "        # 获取文献详细信息\n",
    "        papers = fetch_paper_details(paper_ids)  # 复用之前的fetch函数\n",
    "        \n",
    "        # 构建保存内容\n",
    "        output = [\n",
    "            f\"\\n### {region} & {microbe} ###\",\n",
    "            f\"找到 {len(papers)} 篇相关文献:\"\n",
    "        ]\n",
    "        \n",
    "        for p in papers:\n",
    "            title = p.get(\"TI\", \"无标题\")\n",
    "            pmid = p.get(\"PMID\", \"\")\n",
    "            authors = \"; \".join(p.get(\"AU\", []))[:50] + \"...\"  # 显示前50字符作者\n",
    "            journal = p.get(\"TA\", \"未知期刊\")\n",
    "            year = p.get(\"DP\", \"未知年份\").split()[0]  # 提取年份\n",
    "            \n",
    "            output.append(\n",
    "                f\"\\n标题: {title}\\n\"\n",
    "                f\"PMID: {pmid}\\n\"\n",
    "                f\"链接: https://pubmed.ncbi.nlm.nih.gov/{pmid}/\\n\"\n",
    "                f\"作者: {authors}\\n\"\n",
    "                f\"期刊: {journal} ({year})\"\n",
    "            )\n",
    "        \n",
    "        # 保存结果并打印进度\n",
    "        save_results(\"\\n\".join(output))\n",
    "        print(f\"│ 发现 {microbe} 的 {len(papers)} 篇文献\")\n",
    "\n",
    "print(f\"\\n所有结果已保存至: {SAVE_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlz1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
